<!DOCTYPE html>
<html>
    <head>
        <link href="/theme/css/normalize.css" rel="stylesheet">
        <link href="/theme/css/skeleton.css" rel="stylesheet">
        <link href="/theme/css/statocles-default.css" rel="stylesheet">
        <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">
        <title>Neil H. Watson</title>
        <meta content="Statocles 0.059" name="generator">
        <link href="/blog/index.atom" rel="alternate" type="application/atom+xml">
        <link href="/blog/index.rss" rel="alternate" type="application/rss+xml">
        
    </head>
    <body>
        <header>
            <nav class="navbar">
                <div class="container">
                    <a class="brand" href="/">Neil H. Watson</a>
                    <ul>
                        <li>
                            <a href="/">Blog</a>
                        </li>
                    </ul>
                    
                </div>
            </nav>
            
        </header>
        <div class="main container">
            
<div class="row">

    <div class="nine columns">
        <main>


            <article>
                <header>
                    <h1><a href="/blog/2011/01/25/sec-autid-victory/">Victory over security audits</a></h1>

                    <p class="tags">Tags:
                        <a href="/blog/tag/cfengine/" rel="tag">cfengine</a>
                        <a href="/blog/tag/configuration-management/" rel="tag">configuration management</a>
                        <a href="/blog/tag/infosec/" rel="tag">infosec</a>
                    </p>

                    <aside>
                        <p><time datetime="2011-01-25">
                            Posted on 2011-01-25
                        </time>
                        </p>
                    </aside>

                </header>

                <p>It’s vicious cycle. Resources are spent auditing hosts. Many more are spent fixing all of the audit’s deficiencies. Then it’s back to business as usual. Time passes and hosts slowly degrade until the next audit. Repeat.</p>

<p>It might take an hour to plan, schedule and fix each host. That’s 100 hours for 100 hosts. Audit twice a year and you’ve spent 200 hours. What if you have a 1000 hosts? Using Cfengine you can audit your hosts and fix deficiencies automatically every day! Consider these typical policies.</p>

<pre><code>Disable services that are not required.
No world writable files.
Limited guid and suid files.
Tighten file permissions in selected areas.
Harden services by applying custom configurations.
Ensure certain applications are not installed.
</code></pre>

<p>This can be a lot of manual work. Typically I see these policies applied at build time using services like Jumpstart and Kickstart. But what happens after hosts are built? Over time hosts will diverge from the ideal state set at build time. Cfengine can manage all of these requirements automatically and continuously. As Cfengine makes corrections and reports you’ll build a log of information that the auditors will love. It will show compliance.</p>

<p>Go above and beyond with more advanced policies.</p>

<pre><code>Monitor any file for content changes (e.g. Tripwire).
Monitor host performance for statistical anomalies.
Kill unwanted processes.
Change all local root passwords.
Disable local accounts and ssh keys.
Change any of these policies and all hosts are updated automatically!
</code></pre>

<p>Before you finish your next 300 page audit report defeat this cycle. Find out how you turn audits from headaches into business as usual with Cfengine.</p>



            </article>
            <article>
                <header>
                    <h1><a href="/blog/2007/08/08/clustering-with-redhat/">Clustering with Red Hat</a></h1>

                    <p class="tags">Tags:
                        <a href="/blog/tag/cluster/" rel="tag">cluster</a>
                        <a href="/blog/tag/high-availability/" rel="tag">high availability</a>
                        <a href="/blog/tag/red-hat/" rel="tag">red hat</a>
                    </p>

                    <aside>
                        <p><time datetime="2007-08-08">
                            Posted on 2007-08-08
                        </time>
                        </p>
                    </aside>

                </header>

                <p>This entry presents an overview of how to build a DB2 service cluster using the
Red Hat Cluster Suite. This procedure can be useful in learning how to setup
clusters for other services.</p>

<h2>Contents</h2>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#rh-cluster-suite">Redhat Cluster Software</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#install">Installation</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#config">Configuring the Cluster Suite</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#testing">Testing the Cluster</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#manage">Managing the cluster</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#conclusion">Conclusions</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#refs">References and Additional Reading</a></p>

<p><a href="/blog/2007/08/08/clustering-with-redhat/#appendix-a">DB2 init script</a></p>

<h2><a name="rh-cluster-suite"></a>Redhat Cluster Software</h2>

<p>The Redhat Cluster Suite offers a prepackaged front-end to setup and manage
high availability Linux clusters. Using this method, the cluster nodes share a
single copy of the data. A “fence” is used to ensure that only a single node at
a time is using the data.</p>

<h4>Good</h4>

<ul>
<li>Software is already available at no extra cost.</li>
<li>Redhat support is available at no extra cost.</li>
</ul>

<h4>Bad</h4>

<ul>
<li>If the SAN fails the cluster fails.</li>
<li>All nodes could fail if the shared data is corrupted.</li>
</ul>

<h2><a name="install"></a>Installation</h2>

<p>The chief reference for this procedure is the Redhat Cluster Suite
documentation provided by Redhat (see appendix). It should be read fully in
order to understand this article and to be sure there are no syntax changes
between revisions of this paper and Redhat’s documentation.</p>

<h3>Install Redhat Linux AS 4 on primary node</h3>

<ol>
<li><p>In the /etc/hosts file make entries for each node in the cluster:</p>

<pre><code>172.16.1.203    hadrian.example.com hadrian
172.16.1.204    caesar.example.com caesar
</code></pre></li>
<li><p>Bond two of the four Ethernet cards. Be sure that the network switch is set
to auto negotiate. Forcing protocols can break bonding.</p></li>
<li><p>In /etc/modprobe.conf add these lines:</p>

<pre><code>install bond0 /sbin/modprobe bonding -o bond0 mode=1 miimon=100
</code></pre></li>
<li><p>Edit /etc/sysconfig/network-scripts/ifcfg-eth0 to:</p>

<pre><code>DEVICE=eth0
USERCTL=no
ONBOOT=yes
MASTER=bond0
SLAVE=yes
BOOTPROTO=none
</code></pre></li>
<li><p>Edit /etc/sysconfig/network-scripts/ifcfg-eth1 to:</p>

<pre><code>DEVICE=eth1
USERCTL=no
ONBOOT=yes
MASTER=bond0
SLAVE=yes
BOOTPROTO=none
</code></pre></li>
<li><p>Create the file /etc/sysconfig/network-scripts/ifcfg-bond0 to:</p>

<pre><code>DEVICE=bond0
USERCTL=no
ONBOOT=yes
BROADCAST=176.16.1.255
NETWORK=172.16.1.0
NETMASK=255.255.255.0
GATEWAY=172.16.1.1
IPADDR=172.16.1.203
</code></pre></li>
<li><p>Activate the bonding by restarting the network service. Test for fail over.</p></li>
<li><p>Stop all unneeded services from starting at boot.</p>

<pre><code>chkconfig --del &lt;service&gt;
</code></pre></li>
<li><p>Install the NTP service and point it to an ntp server.</p></li>
<li><p>Install Redhat Cluster Suite by subscribing the system to the channel and
running up2date.</p>

<pre><code>up2date --installall rhel-x86_64-as-cluster
</code></pre></li>
<li><p>Create the shared partition on a SAN. Format it to ext3. Ensure that the
server can mount the shared partition but do not allow the server to mount
this partition automatically. Make fstab entry with the noauto option to
prevent mount and boot time.</p></li>
<li><p>Install Perl SSL module for iLO management</p>

<pre><code>up2date -i perl-Crypt-SSLeay
</code></pre></li>
</ol>

<h3>Install DB2 on primary node</h3>

<p>Certain memory related kernel paramters must be set. Add these lines to
/etc/sysctl.conf. (see Appendix A).</p>

<pre><code>kernel.sem=250 256000 32 1024
#Example shmmax for a 64-bit system
kernel.shmmax=1073741824
#Example shmall for 90 percent of 16 GB memory
kernel.shmall=3774873
kernel.msgmax=65535
kernel.msgmnb=65535
</code></pre>

<p>Unpack required DB2 binaries. These may be different for each install.</p>

<ol>
<li>Run db2install program.</li>
<li>Choose directory /opt/ibm/db2/V9.1.</li>
<li>Choose type “ESE”.</li>
<li>Create group db2fence.</li>
<li>Create group db2inst.</li>
<li>Create user db2fence with db2fence group membership and with the shell
/sbin/nologin.</li>
<li>Create user dwapinst with db2inst membership. Users will access this with
sudo only.</li>
<li>Issue the command:
    db2icrt -a SERVER -p 30001 -s ese -u db2fence dwapinst.</li>
<li>Disable fault monitor to prevent DB2 from spawning without cluster control:
    /opt/ibm/db2/V9.1/bin/db2fmcu -d.</li>
<li>Create an init script to stp and start the DB2 service (see Appendix B).
Ensure that the service does not start at boot time.</li>
</ol>

<h3>Prepare additional nodes</h3>

<p>Repeat the above steps on all participating cluster nodes. Be sure to change
the IP address each time. One could also clone the nodes and then change the IP
addresses and host names afterwards.</p>

<h2><a name="config"></a>Configuring the Cluster Suite</h2>

<p>Configuring the cluster</p>

<ol>
<li>SSH to the primary node using the -YC switches for X forwarding.</li>
<li>Run the system-config-cluster application.</li>
<li>The application will prompt the user to create a new configuration.</li>
<li>Choose the dlm lock method.</li>
<li>Name the cluster “dcdb”.</li>
<li>Create iLO fence devices for each node.</li>
<li>Add all nodes to the cluster. Configure the iLO fence for each node.</li>
<li>Add the shared file system to the resources section. Check the “force
unmount” and the “reboot host node of unmount fails” boxes.</li>
<li>Add the cluster’s floating IP (172.16.1.205) to the resources section. The
floating IP is the IP address that is used to access the service that is
being clustered. The Cluster Suite will assign this IP address to the active
node.</li>
<li>Add the DB2 init script (see Appendix) to the resources section.</li>
<li>Add the DB2 service to the cluster and include all the shared resources
above. Set the recovery policy to “restart”. Check the “autostart this
service” box.</li>
<li>Propagate the cluster configuration by copying the file
/etc/cluster/cluster.conf to each node.</li>
</ol>

<h3>Additional service considerations</h3>

<p>A cluster may have additional services that are not clustered but, must be
aware of the clustering environment. Cron is a good example. Suppose a cron job
has to be run to help maintain the clustered service or to provide some sort of
reporting. If the cron job were to run on a standby node then it would fail. If
the cron job were running on the active node during a fail over what would
happen?</p>

<p>If a cron job has to be run only on the active node then some checks should be
performed before the job is run. For example, the job could check for the
presence of share resources. Is the shared partition mounted? Is the DB2
service running? Multiple checks are better than just one. It may also be
useful to store the scripts on the shared partition. Although cron would call
them, they would not be run on non-active nodes since the mount would not be
valid. Additionally, the job should be able to check for completion and report
any error that may have been caused by fail over.</p>

<p>Finally, remember that service related cron table entries will need to be
duplicated across each node. Other information that may need duplication across
nodes are user accounts and group IDs.</p>

<h2><a name="testing"></a>Testing the Cluster</h2>

<p>Prior to deploying the cluster into service it should be tested to ensure fail
over happens as and when expected. Prepare test cases for the node. The current
expected results for each test are only estimates. Proper testing will clarify
how the cluster suite manages these problems.</p>

<h3>Turn off the network</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster.</p>

<h4>Actual results</h4>

<p>The cluster’s standby node activated in about 2 minutes. Within 4 minutes the
failed node had been rebooted and instated as the standby node.</p>

<h3>Unmount the shared partition</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster.</p>

<h4>Actual results</h4>

<p>The cluster remounted the shared partition and restarted the DB2 service. The
restart was completed in less than one minute.</p>

<h3>Stop DB2</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster. The status of the cluster can be tested with:
clustat.</p>

<h4>Actual results</h4>

<p>The cluster stops all the shared resources, including DB2 even though it is
already stopped. The cluster then restarted all the shared resources on the
same node. The service was returned to normal in under one minute.</p>

<h3>Unplug a single network cable</h3>

<h4>Expected results</h4>

<p>Since two Ethernet cards are bonded in fail over mode this should not impact
service at all. The kernel should start the standby card within a few seconds.</p>

<h4>Actual results</h4>

<p>The results were as expected.</p>

<h3>Shutdown node</h3>

<p>The node is powered off without issuing a software shutdown.</p>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster. The status of the cluster can be tested with:
clustat.</p>

<h4>Actual results</h4>

<p>The cluster activated the standby node within two minutes. The cluster, using
the iLO fence, turned the power on to the failed node. The node then booted as
normal and was returned to the cluster as a standby node.</p>

<h3>Unplug fiber cable</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster.</p>

<h4>Actual results</h4>

<p>The cluster failed over as expected. Once the fiber cable was plugged again in
the failed node was returned to the cluster as a standby node.</p>

<h3>Turn off the network during a DB2 write</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster.</p>

<h4>Actual results</h4>

<p>The cluster failed over to the standby node in less than two minutes. The
failed node was then rebooted and returned to standby service automatically.
The data being written to DB2 was completed up to the last commit. Uncommitted
data was lost.</p>

<h3>Unplug fiber cable during a DB2 write</h3>

<h4>Expected results</h4>

<p>The cluster should fail over to the standby node. The elapsed time for this may
impact the SLA for the cluster.</p>

<h4>Actual results</h4>

<p>The cluster moved the service to the standby node. The previously active node
was left as is. The data being written to DB2 was complete up to the last
commit. Uncommitted data was lost.</p>

<h2><a name="manage"></a>Managing the cluster</h2>

<p>During the life of the cluster, the administrator will need to perform certain
tasks regularly. The following is a list of those tasks and how to complete
them.</p>

<h3>Manual fail over</h3>

<p>Bringing a node down for maintenance is a normal operation that allows
administrators to maintain servers without affecting service. Also, such
maintenance can be performed during normal working hours.</p>

<h3>Fail over node</h3>

<p>If the node to be brought down is the currently running node then the service
must be moved to a standby node. On the running node issue this command:
        /usr/sbin/clusvcadm -r db2 -m caesar
This instructs the cluster to move the service (db2) to another node (caesar).
The cluster will then stop the service and restart it on the other node.
Downtime while the service is moved should less than thirty seconds.</p>

<h3>Remove node from cluster</h3>

<p>The node that is now down must be marked out of service to the cluster. This
will prevent the cluster from attempting to use it if the running node should
fail. Stop these services, in order, and then ensure that they do not start at
boot time:</p>

<pre><code>    service rgmanager stop
    service fenced stop
    service cman stop
    service ccsd stop

    chkconfig --del rgmanager
    chkconfig --del fenced
    chkconfig --del cman
    chkconfig --del ccsd
</code></pre>

<h3>Perform off line maintenance</h3>

<p>Now the node is now not part of the cluster. It can be upgraded, rebooted or
anything else that is required without affecting the cluster.</p>

<h3>Return node to cluster</h3>

<p>The node must now be added back to the cluster.</p>

<pre><code>chkconfig --add rgmanager
chkconfig --add fenced
chkconfig --add cman
chkconfig --add ccsd

service ccsd start
service cman start
service fenced start
service rgmanager start
</code></pre>

<h3>Checking cluster status</h3>

<p>The current status of the cluster can be checked in two ways.</p>

<h4>X Windows GUI</h4>

<p>You must have a working X server to use this option.</p>

<ol>
<li>SSH to any node using the -YC switches for X forwarding.</li>
<li>Run the system-config-cluster application.</li>
</ol>

<h4>Command line interface</h4>

<p>The command clustat will return the status of the cluster. For example:</p>

<pre><code>    [root@hadrian ~]# clustat
    Member Status: Quorate

      Member Name                  Status
      ------ ----                  ------
      hadrian                      Online, Local, rgmanager
      caesar                       Online, rgmanager

      Service Name   Owner (Last)             State         
      ------- ----   ----- ------             -----         
      db2            hadrian                  started
</code></pre>

<h3>Log files</h3>

<p>The cluster suite logs events in the file /var/log/messages. The cluster logs
status events and fail overs. For example:</p>

<h4>Status check</h4>

<pre><code>    Aug 16 15:09:40 caesar clurgmgrd: [5159]: &lt;info&gt; Executing
    /etc/rc.d/init.d/db2 status 
    Aug 16 15:09:40 caesar su(pam_unix)[28369]: session opened for user dwapinst by
    (uid=0)
    Aug 16 15:09:40 caesar su: 
    Aug 16 15:09:40 caesar su: Instance  : dwapinst
    Aug 16 15:09:40 caesar su: DB2 State : Available
    Aug 16 15:09:40 caesar su(pam_unix)[28369]: session closed for user dwapinst
    Aug 16 15:09:40 caesar db2:  succeeded
</code></pre>

<h4>Failover</h4>

<pre><code>    Aug 16 15:03:26 hadrian kernel: end_request: I/O error, dev sda, sector 65
    Aug 16 15:03:26 hadrian kernel: EXT3-fs: unable to read superblock
    Aug 16 15:03:26 hadrian clurgmgrd: [4679]: &lt;err&gt; &#39;mount -t ext3
    /dev/sda1 /db2&#39; failed, error=32
    Aug 16 15:03:26 hadrian clurgmgrd[4679]: &lt;notice&gt; start on fs &quot;db2&quot;
    returned 2 (invalid argument(s))
    Aug 16 15:03:26 hadrian clurgmgrd[4679]: &lt;warning&gt; #68: Failed to start
    db2; return value: 1
    Aug 16 15:03:26 hadrian clurgmgrd[4679]: &lt;notice&gt; Stopping service db2
    Aug 16 15:03:26 hadrian clurgmgrd: [4679]: &lt;info&gt; Executing
    /etc/rc.d/init.d/db2 stop
    Aug 16 15:03:26 hadrian su(pam_unix)[27368]: session opened for user dwapinst
    by (uid=0)
    Aug 16 15:03:26 hadrian su:
    Aug 16 15:03:26 hadrian su: Instance  : dwapinst
    Aug 16 15:03:26 hadrian su: DB2 State : Operable
    Aug 16 15:03:26 hadrian su(pam_unix)[27368]: session closed for user dwapinst
    Aug 16 15:03:26 hadrian db2:  failed
    Aug 16 15:03:26 hadrian db2:  succeeded
    Aug 16 15:03:26 hadrian clurgmgrd: [4679]: &lt;info&gt; /dev/sda1 is not
    mounted
    Aug 16 15:03:31 hadrian clurgmgrd[4679]: &lt;notice&gt; Service db2 is
    recovering
    Aug 16 15:03:31 hadrian clurgmgrd[4679]: &lt;warning&gt; #71: Relocating failed
    service db2
    Aug 16 15:03:35 hadrian clurgmgrd[4679]: &lt;notice&gt; Service db2 is now
    running on member 1
</code></pre>

<h3><a name="conslusion"></a>Conclusions</h3>

<p>The Redhat Cluster Suite can offer a significant amount of redundancy above
that of a single, well built server. Additionally, down time is reduced since
cluster nodes can be maintained at leisure with little impact on service
delivery. A chain is only as strong as its weakest link. An important proverb
when building a cluster. While the nodes are redundant, external hardware may
not be. Are all the nodes connected to the same network switch? Are they
connected to the same UPS? Do the nodes rely on the same gateway router? The
cluster in this paper does not include a redundant infrastructure</p>

<p>It should also be noted that the client must be able to account for a fail
over. During the write tests, some data was lost. The client must be able to
confirm its writes, know when a failure has occurred and be able to continue
after its last success. Similarly, if a client is reading from the cluster it
should be able to, where possible, detect a connection loss and repeat the
request.</p>

<p>Although this paper discusses a DB2 cluster, it could be easily applied to
cluster other services like Apache or Bind. It is even possible to have a
cluster provide more than one service. Clusters can also be configured with
load balancing instead of or, in addition to redundancy.</p>

<h3><a name="refs"></a>References and Additional Reading</h3>

<ol>
<li><a href="http://www.redhat.com/docs/manuals/csgfs/browse/rh-cs-en/index.html">Redhat Cluster Suite</a></li>
<li><a href="http://www.linux-ha.org/">High Availability Linux</a></li>
<li><a href="http://publib.boulder.ibm.com/infocenter/db2luw/v9/index.jsp?topic=/com.ibm.db2.udb.uprun.doc/doc/t0008238.htm">Kernel Parameters for Linux</a></li>
</ol>

<h3><a name="appendix-a"></a>DB2 init script</h3>

<pre><code>    #!/bin/bash
    #
    #-------------------------  /etc/init.d/db2 ------------------------
    # db2 
    #
    # description:  Start up the db2 service 

    # Source function library.
            . /etc/rc.d/init.d/functions

    # Application owner 
    USER=dwapinst

    RETVAL=0
    prog=&quot;db2&quot;

    start() {
        echo -n $&quot;Starting $prog:&quot;
        initlog -c &quot;/bin/su - $USER -c &#39;db2start&#39;&quot; &amp;&amp; success || failure
        RETVAL=$?
        echo &quot;&quot;

    }

    status() {
        initlog -c &quot;/bin/su - $USER -c &#39;db2gcf -s&#39;&quot; &amp;&amp; success || failure
        RETVAL=$?
        echo &quot;&quot;
    }

    stop() {
        echo -n $&quot;Stopping $prog:&quot;

        # Is DB2 already stopped?
        status
        if [ $RETVAL -gt 0 ]; then

            # Already stoped return 0
            echo &quot;Already stopped&quot;
            success
            RETVAL=0
            echo &quot;&quot;
        else
            # DB2 must still be running.  Stop it.
            initlog -c &quot;/bin/su - $USER -c \
                &#39;db2 force application all&#39;&quot; \
                &amp;&amp; success || failure &amp;
            sleep 5
            initlog -c &quot;/bin/su - $USER -c \
                &#39;db2stop force&#39;&quot; &amp;&amp; success \
                || failure &amp;
            sleep 5
            initlog -c &quot;/bin/su - $USER -c \
                &#39;db2_kill&#39;&quot; &amp;&amp; success \
                || failure &amp;
            echo &quot;&quot;
        fi
    }

    case &quot;$1&quot; in
     start)
       start
            ;;
     stop)
            stop 
            ;;

     status)
            status
            ;;

     restart)
       stop
            sleep 3
       start
            ;;
     *)
        echo $&quot;Usage: $0 {start|stop|restart}&quot;
        RETVAL=1
        ;;
    esac

    exit $RETVAL
</code></pre>



            </article>
        </main>

        <ul class="pager">
            <li class="prev">
                <a class="button disabled" href="" rel="next">
                    ← Older
                </a>
            </li>
            <li class="next">
                <a class="button button-primary" href="/blog/page/7" rel="prev">
                    Newer →
                </a>
            </li>
        </ul>

    </div>

    <div class="three columns sidebar">
        
        <h1>Tags</h1>
        <ul class="list-inline">
            <li><a href="/blog/tag/apt/">apt</a></li>
            <li><a href="/blog/tag/backup/">backup</a></li>
            <li><a href="/blog/tag/cfengine/">cfengine</a></li>
            <li><a href="/blog/tag/cfengine-cookbook/">cfengine cookbook</a></li>
            <li><a href="/blog/tag/cheatsheet/">cheatsheet</a></li>
            <li><a href="/blog/tag/cli/">cli</a></li>
            <li><a href="/blog/tag/cluster/">cluster</a></li>
            <li><a href="/blog/tag/configuration-management/">configuration management</a></li>
            <li><a href="/blog/tag/cron/">cron</a></li>
            <li><a href="/blog/tag/debian/">debian</a></li>
            <li><a href="/blog/tag/dns/">dns</a></li>
            <li><a href="/blog/tag/dr/">dr</a></li>
            <li><a href="/blog/tag/heartbeat/">heartbeat</a></li>
            <li><a href="/blog/tag/high-availability/">high availability</a></li>
            <li><a href="/blog/tag/infosec/">infosec</a></li>
            <li><a href="/blog/tag/ipv6/">ipv6</a></li>
            <li><a href="/blog/tag/iscsi/">iscsi</a></li>
            <li><a href="/blog/tag/kvm/">kvm</a></li>
            <li><a href="/blog/tag/linux/">linux</a></li>
            <li><a href="/blog/tag/monitoring/">monitoring</a></li>
            <li><a href="/blog/tag/multipathd/">multipathd</a></li>
            <li><a href="/blog/tag/networking/">networking</a></li>
            <li><a href="/blog/tag/opensource/">opensource</a></li>
            <li><a href="/blog/tag/oracle/">oracle</a></li>
            <li><a href="/blog/tag/posix/">posix</a></li>
            <li><a href="/blog/tag/red-hat/">red hat</a></li>
            <li><a href="/blog/tag/regex/">regex</a></li>
            <li><a href="/blog/tag/rhev/">rhev</a></li>
            <li><a href="/blog/tag/san/">san</a></li>
            <li><a href="/blog/tag/shell/">shell</a></li>
            <li><a href="/blog/tag/solaris/">solaris</a></li>
            <li><a href="/blog/tag/ssh/">ssh</a></li>
            <li><a href="/blog/tag/subversion/">subversion</a></li>
            <li><a href="/blog/tag/sun/">sun</a></li>
            <li><a href="/blog/tag/support/">support</a></li>
            <li><a href="/blog/tag/svn/">svn</a></li>
            <li><a href="/blog/tag/sysadmin/">sysadmin</a></li>
            <li><a href="/blog/tag/vitualization/">vitualization</a></li>
        </ul>
        <h1>Feeds</h1>
        <ul class="list-inline">
            <li>
                <a href="/blog/index.atom" rel="alternate" type="application/atom+xml">
                    Atom
                </a>
            </li>
            <li>
                <a href="/blog/index.rss" rel="alternate" type="application/rss+xml">
                    RSS
                </a>
            </li>
        </ul>
    </div>

</div>


        </div>
        <footer>
            
            <div class="container tagline">
                <a href="http://preaction.github.io/Statocles">Made with Statocles</a><br>
                <a href="http://www.perl.org">Powered by Perl</a>
            </div>
        </footer>
    </body>
</html>
