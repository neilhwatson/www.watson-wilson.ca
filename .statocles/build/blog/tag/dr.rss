<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Neil H. Watson</title>
        <link>http://ettin/blog/tag/dr/</link>
        <atom:link href="http://ettin/blog/tag/dr.rss" rel="self" type="application/rss+xml" />
        <description>Blog feed of Neil H. Watson</description>
        <generator>Statocles 0.059</generator>
        <item>
            <title>Backups using Cfengine</title>
            <link>http://ettin/blog/2011/10/02/cfengine-backups/</link>
            <guid>http://ettin/blog/2011/10/02/cfengine-backups/</guid>
            <description><![CDATA[
                <p>Problem</p>

<p>You want to use Cfengine for scheduling backups instead of crond.</p>

<p>Solution</p>

<p>Cfengine is flexible enough to allow for advanced time scheduling (see Cfengine as an atlernative to crond). Its file copying functions allow Cfengine to copy files directly as a backup program including advanced rsync behaviour. Cfengine shell commands allow you to use your own backup tools.</p>

<p>Earlier I discussed methods promises. I will make use of this again here.
The humble tar ball</p>

<p>First a method call.</p>

<p>bundle agent main {</p>

<p>vars:
      &quot;backupdir&quot; string =&gt; &quot;/srv/aux/backup&quot;,
         handle =&gt; &quot;main_vars_backupdir&quot;,
         comment =&gt; &quot;Location for backups&quot;;</p>

<pre><code>  &quot;backups&quot; slist =&gt; {
        &quot;/home/neil/.kde/share/apps/amarok&quot;,
        &quot;/home/neil/.ssh&quot;,
        &quot;/home/neil/.gnupg&quot;,
        &quot;/home/neil/.gnucash&quot;,
        &quot;/home/neil/Mail&quot;,
        &quot;/srv/music/playlists&quot;,
        &quot;/etc&quot;,
        &quot;/var/www&quot;,
        &quot;/srv/svn/&quot;
     },
     handle =&gt; &quot;vars_home_backups&quot;,
     comment =&gt; &quot;Dirs to backup&quot;;
</code></pre>

<p>methods:</p>

<pre><code>  Hr01::

     &quot;backup&quot; usebundle =&gt; backup( 
           ${backupdir}, 
           @{main.backups} ),
        action =&gt; if_elapsed(&quot;60&quot;),
        depends_on =&gt; { &quot;svn_checkin&quot; },
        handle =&gt; &quot;main_methods_home_backups&quot;;
</code></pre>

<p>}</p>

<p>Here we set some variables for a backup directory and a list of things to backup. Doing this allows us to change the variables by class from host to host. Next we call the bundle via a method passing the variables. Note that list variables must be fully scoped by listing their bundle name.</p>

<p>The bundle is called at 0100 hours. If_elapsed ensures that the backup in only promised once during that hour. Depends_on shows what other bundles might need to be done first. In this case I need a Subversion promise ahead of time.</p>

<p>The magic here is that not only will the bundle go off at 0100 but like all Cfengine promises it will continually try, for every run during that hour, to ensure that that promise is kept. Now let’s look at the called bundle.</p>

<p>bundle agent backup(dir, backups) {</p>

<p>vars:</p>

<p>&quot;dirs&quot; string =&gt; join(&quot; &quot;, &quot;backups&quot;),
      handle =&gt; &quot;backup_vars_dirs&quot;,
      comment =&gt; &quot;Make list of string for tar&quot;;</p>

<p>files:</p>

<p>&quot;${dir}/.&quot;
      handle =&gt; &quot;backup_files_srv_backups&quot;,
      create =&gt; &quot;true&quot;;</p>

<p>commands:</p>

<p>&quot;/bin/tar -czf ${dir}/cfbackup-${g.day}.tgz ${dirs}&quot;
      handle =&gt; &quot;backup_commands_tar&quot;,
      contain =&gt; silent,
      classes =&gt; if_else(&quot;backup_go_encrypt&quot;,&quot;backup_tar_failed&quot;);</p>

<p>reports:</p>

<p>backup_tar_failed::
      &quot;Backup tar command failed.&quot;; 
}</p>

<p>The bundle is quite simple. A tar command is run using the passed variables as a target for the tar file and a list of things to tar up. The variable ‘g.day’ is a global variable that always contains a three letter abbreviation of today (e.g. sun, mon, tue). This is not built in. I set this in a common bundle. The files promise ensure that the location for the tar file is always there. The reports promise reports if the tar command throws and error. This would not be necessary in Nova since it keeps track of promise compliance automatically.</p>

<p>A peek inside this backup directory shows:</p>

<h1>ls /srv/aux/backup/cfbackup*</h1>

<p>/srv/aux/backup/cfbackup-fri.tgz
/srv/aux/backup/cfbackup-sun.tgz
/srv/aux/backup/cfbackup-wed.tgz
/srv/aux/backup/cfbackup-mon.tgz
/srv/aux/backup/cfbackup-thu.tgz
/srv/aux/backup/cfbackup-sat.tgz
/srv/aux/backup/cfbackup-tue.tgz</p>

<p>A simple copy</p>

<p>I have a directory of images from my camera work. I like to back this up to another hard drive on my workstation. First the method.</p>

<p>methods:</p>

<p>Hr01::</p>

<pre><code>  &quot;backup_pictures&quot; usebundle =&gt; local_sync(
        &quot;/home/neil/Pictures&quot;,
        &quot;/srv/aux/backup-pictures&quot;
     ),
     action =&gt; if_elapsed(&quot;10080&quot;),
     handle =&gt; &quot;main_methods_local_sync_pictures&quot;;
</code></pre>

<p>This method passes two strings, both directories, to a bundle. The if_elapsed body sets the frequency of this backup to once per week. Now the bundle.</p>

<p>bundle agent local_sync(src, dest){</p>

<p>files:</p>

<p>&quot;${dest}/.&quot;
      handle =&gt; &quot;local_sync_files_dest_create&quot;,
      create =&gt; &quot;true&quot;,
      perms =&gt; mog(&quot;644&quot;,&quot;root&quot;,&quot;root&quot;);</p>

<p>&quot;${dest}&quot;
      handle =&gt; &quot;local_sync_files_dest_copy&quot;,
      depth_search =&gt; recurse(&quot;inf&quot;),
      copy_from =&gt; local_cp(&quot;${src}&quot;);
}</p>

<p>Two files promises are found here. The first ensure that the destination directory is available. The second copies files from the source, provided by the method above to the destination, also provided by the method. The copy_from body is found in the standard Cfengine library. This is just a straight copy. I could have used a purge in the copy_from body to perform a sync function instead.
Encryption and off-site backups</p>

<p>Some of my backups are sent to an off-site host. I don&#39;t fully trust the off-site location so I encrypt the backup files first. Again from my main bundle.</p>

<p>methods:</p>

<p>&quot;backup_encrypt_neil&quot; usebundle =&gt; backup_encrypt_neil(&quot;cfbackup-${g.day}.tgz&quot;),
      action =&gt; if_elapsed(&quot;60&quot;),
      handle =&gt; &quot;main_methods__backup_encrypt_neil_cfbackup&quot;;</p>

<p>oort.(Saturday|Wednesday).(Hr10|Hr11)::</p>

<pre><code>  &quot;remote_backup&quot; usebundle =&gt; remote_backup_vps,
     handle =&gt; &quot;main_methods_oort_remote_backup_vps&quot;,
     comment =&gt; &quot;CEST time zone&quot;,
     action =&gt; if_elapsed(&quot;120&quot;);
</code></pre>

<p>The first method calls a bundle make encrypted copies of my backup tar balls. The second method is invoked on a remote host called ‘oort’. It is invoked on Saturday or Wednesday during the hours of 1000 or 1100 local time. The if_elapsed sets the span of two hours to ensure the promise is kept. Now the bundles.</p>

<p>bundle agent backup_encrypt_neil(file) {</p>

<p>vars:</p>

<p>&quot;backup_dir&quot; string =&gt; &quot;/srv/aux/backup&quot;,
      handle =&gt; &quot;backup_encrype_neil_backup_dir&quot;;</p>

<p>&quot;encrypted_dir&quot; string =&gt; &quot;/srv/aux/backup/encrypted&quot;,
      handle =&gt; &quot;backup_encrype_neil_encrypted_dir&quot;;</p>

<p>files:</p>

<p>&quot;${encrypted_dir}/.&quot;
      handle =&gt; &quot;backup_encrypt_neil_files_encrypted&quot;,
      create =&gt; &quot;true&quot;;</p>

<p>commands:</p>

<p>&quot;/usr/bin/gpg -r &#39;Neil Watson&#39; -o  ${encrypted_dir}/${file}.en -e ${backup_dir}/${file}&quot;,
      contain =&gt; silent,
      handle =&gt; &quot;backup_encrypt_neil_commands_encrypt&quot;;
}</p>

<p>This bundle calls GNUpg via a commands promise to encrypt the given tar file. A files promise ensures that the directory for the encrypted file to reside exists.</p>

<p>A peek inside the encrypted directory shows:</p>

<h1>ls /srv/aux/backup/encrypted/cfbackup*</h1>

<p>/srv/aux/backup/encrypted/cfbackup-fri.tgz.en
/srv/aux/backup/encrypted/cfbackup-thu.tgz.en
/srv/aux/backup/encrypted/cfbackup-mon.tgz.en
/srv/aux/backup/encrypted/cfbackup-tue.tgz.en
/srv/aux/backup/encrypted/cfbackup-sat.tgz.en
/srv/aux/backup/encrypted/cfbackup-wed.tgz.en
/srv/aux/backup/encrypted/cfbackup-sun.tgz.en</p>

<p>Now the remote backup bundle.</p>

<p>bundle agent remote_backup_vps {</p>

<p>files:</p>

<p>&quot;/home/neil/backups&quot;
      handle =&gt; &quot;remote_backup_vps_files_backups&quot;,
      depth_search =&gt; recurse(&quot;inf&quot;),
      file_select =&gt; by_name(&quot;.<em>?${g.day}.</em>?.en&quot;),
      copy_from =&gt; remote_dcp(
         &quot;/srv/aux/backup/encrypted&quot;,
         &quot;${sys.policy_hub}&quot;
      );
}</p>

<p>This last bundle is another files promise that performs a copy. The file_select body ensure that only the tar file that matches today’s tar file is copied. The thing to remember with this type of operation is that the server listed in the copy_from body is whatever host houses the backups. In this case it is my policy hub. However it could be any host running Cfengine so long as the proper access rules are applied.</p>

<p>Should you be using Nova you can use something called remote classes. Remote classes are classes that are set on one client so that other clients can query it. In this example I could have the host that encrypts the files set a class called ‘encrypted_files_ready’. The remote host could query for this class. If it is set then kickoff of the remote backup bundle.</p>

                    <p><a href="http://ettin/blog/2011/10/02/cfengine-backups/#section-2">Continue reading...</a></p>
                <p>Tags:
                    <a href="http://ettin/blog/tag/cfengine/">cfengine</a>
                    <a href="http://ettin/blog/tag/cfengine-cookbook/">cfengine cookbook</a>
                    <a href="http://ettin/blog/tag/backup/">backup</a>
                    <a href="http://ettin/blog/tag/dr/">dr</a>
                </p>
            ]]></description>
            <pubDate>
                Sun, 02 Oct 2011 00:00:00 +0000
            </pubDate>
        </item>
        <item>
            <title>Disaster recovery for small business</title>
            <link>http://ettin/blog/2011/07/14/dr-for-soho/</link>
            <guid>http://ettin/blog/2011/07/14/dr-for-soho/</guid>
            <description><![CDATA[
                <p>Not everyone can afford redundant servers and sites. If you are a small business here are some off site disaster recovery options to think about.</p>

<p>Where and how do you backup your files now? Do you take these backups off site? One option I use is to send backups to a hosted virtual private server or VPS. Many companies provide both Linux and Windows VPS options for as little at $10 per month. Another option is to use services like Dropbox. Either option allows you to upload backup files to a safe remote location. Before you do this there is one extra step that you must consider.</p>

<p>Once your data resides on the remote host, who has access to it? The buzz word of the day is the cloud. You cannot control who at the provider can gain access to your backups. The solution to this is encryption. Encrypt the backup files before you send them to the VPS. For example, suppose I have one Linux host. I perform my backups with Tar. I want to use GnuPG to encrypt them. Consult with the GnuPG documentation on how to generate a key pair. Once you have done so you can encrypt your backups like this.</p>

<p>gpg -r &quot;Neil Watson&quot; -o - -e mybackup.tar.gz |    ssh vpn.example.com &quot;cat &gt; backups/mybackup.tar.gz.en&quot;</p>

<p>This instructs GnuGP to encrypt my backup tar file. The encrypted file is sent to standard output. This is piped to SSH where it is redirected to a target file at my remote VPS.</p>

<p>When you need this backup how will you recover it? You’ll need to download the encrypted file. Then you’ll need to decrypt the file. Suppose, during the disaster you’ve also lost your GnuPG private key? The trick is record this information and back it up separately. Record your VPS credentials and the method you need to access it. This might include the IP address or URL. Record the GnuGP private key to ascii using the armor option. This will allow you import the key anew if lost.</p>

<p>This data must be stored in a safe place. Multiple copies and locations are encouraged. Storage methods could include both paper and CDROM or USB key. Store this data off site. A safety deposit box is ideal.</p>

<p>If this article has you thinking you’ll realize that there are other ways to do this. By all means use them. Just remember to be sure that your data is protected and that your recovery method is stored in a safe place.</p>

<p>If you want my help with your disaster recovery plans please contact me.</p>

                    <p><a href="http://ettin/blog/2011/07/14/dr-for-soho/#section-2">Continue reading...</a></p>
                <p>Tags:
                    <a href="http://ettin/blog/tag/dr/">dr</a>
                    <a href="http://ettin/blog/tag/backup/">backup</a>
                </p>
            ]]></description>
            <pubDate>
                Thu, 14 Jul 2011 00:00:00 +0000
            </pubDate>
        </item>
    </channel>
</rss>

